# Project: Data Modeling with PostgreSQL
This is the first project of Udacity's Data Engineer Nanodegree. It will apply:

- Modeling with Postgres
- Building an ETL using Python
- Defining a star schema

## Introduction
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Project Description
In this project, you'll apply what you've learned on data modeling with Postgres and build an ETL pipeline using Python. To complete the project, you will need to define fact and dimension tables for a star schema for a particular analytic focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.

## Datasets
There are 2 datasets for this project.

The first dataset (song dataset) is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song.

The second dataset (log dataset) consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

## Data Model
The data model used in this project is the Star Schema. There is 5 tables in total, being one fact table and four dimension tables.

### Fact Table

***Table songplays***: Records in log data associated with song plays i.e. records with page NextSong.

- songplay_id INT PRIMARY KEY
- start_time BIGINT
- user_id INT
- level VARCHAR
- song_id VARCHAR
- artist_id VARCHAR
- session_id INT
- location VARCHAR
- user_agent VARCHAR

### Dimension Tables

***Table users***: users in the app.

- user_id INT PRIMARY KEY
- first_name VARCHAR
- last_name VARCHAR
- gender VARCHAR
- level VARCHAR

***Table songs***: songs in music database.

- song_id VARCHAR PRIMARY KEY
- title VARCHAR
- artist_id VARCHAR
- year INT
- duration INT

***Table artists***: artists in music database.

- artist_id VARCHAR PRIMARY KEY
- name VARCHAR
- location VARCHAR
- latitude FLOAT
- longitute FLOAT

***Table time***:timestamps of records in songplays broken down into specific units.

- start_time TIMESTAMP PRIMARY KEY
- hour INT
- day INT
- week INT 
- month INT
- year INT
- weekday INT

## Project Files

1. ***test.ipynb*** displays the first few rows of each table to let you check your database.
2. ***create_tables.py*** drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
3. ***etl.ipynb*** reads and processes a single file from ***song_data*** and ***log_data*** and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.
4. ***etl.py*** reads and processes files from ***song_data*** and ***log_data*** and loads them into your tables. You can fill this out based on your work in the ETL notebook.
5. ***sql_queries.py*** contains all your sql queries, and is imported into the last three files above.
6. ***README.md*** provides discussion on your project.

## Project Steps

### Creating Tables
1. Write CREATE, DROP and INSERT statements in ***sql_queries.py*** to create each table.
2. Run on the terminal ***create_tables.py*** to create your database and tables.
```python3 create_tables.py```
3. Run ***test.ipynb*** to confirm the creation of your tables with the correct columns.

### Building ETL Processes
1. Follow instructions in the ***etl.ipynb*** notebook to develop ETL processes for each table. At the end of each table section
2. Run ***test.ipynb*** to confirm that records were successfully inserted into each table. 
3. Remember to rerun ***create_tables.py*** to reset your tables before each time you run this notebook.

### Buildings ETL Pipeline
1. Complete ***etl.py***, where you'll process the entire datasets.
- Open all the files in dataframes using pandas (pd.read_jason)
- Select the fields that we will need and insert the data
> song_data = list(df[['song_id', 'title', 'artist_id', 'year', 'duration']].values[0])
> artist_data = list(df[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']].values[0])
- Filter songs by NextSong action
>  df = df[df['page']=='NextSong']
- Convert start_time as timestamp in millisencs to datetime format
> t = pd.to_datetime(df['ts'], unit='ms')
- Load and insert data to users table
- Implement the ***song_select*** query in ***sql_queries.py*** to find the song ID and artist ID based on the title, artist name, and duration of a song.
> song_select = ("""
SELECT song_id, songs.artist_id
FROM songs
JOIN artists
ON songs.artist_id = artists.artist_id
WHERE songs.title = (%s) AND artists.name = (%s) AND songs.duration = (%s)
""")
2. Remember to run ***create_tables.py*** before running ***etl.py*** on the terminal to reset your tables.
> python3 create_tables.py
> python3 etl.py
3. Run ***test.ipynb*** to confirm your records were successfully inserted into each table.
